{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29814e2",
   "metadata": {
    "id": "f29814e2"
   },
   "source": [
    "**Part II**\n",
    "\n",
    "In this part, we focus on convolutional layers, which are the basic building blocks of classical architectures such as VGG and ResNet.\n",
    "To understand their effect, we manipulate them a bit and introduce the maxpooling operations often associated with them (**A**). Then, we observe how their parameters have converged after training on the ImageNet database (**B**). We also visualize the output signal of the convolutional layers (**C**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e9b3",
   "metadata": {
    "id": "2dc7e9b3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn   # pre-defined layers\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# to load pre-trained models on ImageNet:\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429e4f2",
   "metadata": {
    "id": "0429e4f2"
   },
   "source": [
    "We will also need to access the shared folder. To do this, run drive.mount, click on the link, and enter the password."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f5b38",
   "metadata": {
    "id": "945f5b38"
   },
   "source": [
    "**A.** Convolutional Layers and Maxpooling\n",
    "\n",
    "Before defining the convolutional layers, let's examine two standard deep learning models (vgg16 and resnet50). In the final layers, you will recognize perceptrons (\"classifier\"), formed from the nn.Linear class. \\\\\n",
    "But, in the first part of the network, you see the 'conv2d' layers that correspond to these famous convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503d00c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8503d00c",
    "outputId": "9fa73be7-6970-4189-bdfc-1b345676fe50"
   },
   "outputs": [],
   "source": [
    "# Two deep neural networks: VGG16 and ResNet50\n",
    "\n",
    "model = models.vgg16(pretrained=False)\n",
    "# model = models.resnet50(pretrained=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25587e81",
   "metadata": {
    "id": "25587e81"
   },
   "source": [
    "In the simplest case, in 1 dimension, the \"convolution\" of a neuron is defined by a relation of the form:\n",
    "\\begin{equation*}\n",
    "output_i = bias + \\sum_{j = 1}^n input_{i + j} \\times kernel_{j}  \\tag{1}\n",
    "\\end{equation*}\n",
    "Here, $kernel$ represents a vector of size $n$ containing the parameters of the neuron. If, for example, $kernel$ is positive and has a sum of 1, it is a moving average. Finally, note that the classic convolution operator differs slightly ($input_{i - j}$ instead of $input_{i + j}$).\n",
    "\n",
    "The PyTorch code is more complicated because:\n",
    "- the input is generally a batch of images with multiple channels\n",
    "- there is not just one neuron\n",
    "\n",
    "The general form is defined [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d).\n",
    "\n",
    "On this page, $C_{in}$, $C_{out}$, and $N$ correspond respectively to the numbers of input channels, layer neurons, and batch size.\n",
    "\n",
    "The operator $\\star$ also hides subtleties: you need to set how the kernel moves on the input (*stride*) and deal with border issues (*padding*). These aspects, which we will not dwell on, are easier to understand from the animations by [Vincent Dumoulin](https://github.com/vdumoulin/conv_arithmetic).\n",
    "\n",
    "To illustrate the effect of a 2D convolutional layer with one neuron, let's load an RGB image.\n",
    "\n",
    "**Upload the file 'cat.jpg' from the GitHub repository to Google Colab by dragging and dropping it into the Files Explorer on the left side.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M6o8qZZvFq9E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6o8qZZvFq9E",
    "outputId": "f4400529-f65a-466c-920f-fda7e851fcfb"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/relmonta/ml-student.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CBPvY-3eF6xL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBPvY-3eF6xL",
    "outputId": "044e458b-7209-4465-bd33-1fe85dc1d38b"
   },
   "outputs": [],
   "source": [
    "! ls ml-student/TP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64997eda",
   "metadata": {
    "id": "64997eda"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "root = 'ml-student/TP1'\n",
    "path = os.path.join(root, 'cat.jpg')\n",
    "image = Image.open(path).convert(\"RGB\")\n",
    "image = image.resize((256,256))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e23ad3",
   "metadata": {
    "id": "34e23ad3"
   },
   "source": [
    "The \"convolution\" operators used by the `Conv2d` class are coded in the `torch.nn.functional` module. In the following lines, we define a Gaussian kernel that can then be applied to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fca095",
   "metadata": {
    "id": "03fca095"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # convolution layers\n",
    "\n",
    "# Definition of a Gaussian kernel of size 10\n",
    "x_range = torch.arange(-5, 5, 0.5)\n",
    "y_range = torch.arange(-5, 5, 0.5)\n",
    "xx, yy = torch.meshgrid(x_range, y_range)\n",
    "var = 10\n",
    "\n",
    "kernel =  (1./(2.*3.14*var)) * torch.exp( - (xx**2 + yy**2)/(2*var) )  # Gaussian\n",
    "\n",
    "plt.imshow(kernel, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8e9bf",
   "metadata": {
    "id": "ebf8e9bf"
   },
   "outputs": [],
   "source": [
    "# Applying the same kernel to all channels:\n",
    "kernel = kernel.repeat(1,3,1,1)                        # repeat for each R,G,B channel\n",
    "\n",
    "# We added at the beginning the dimension associated with the image indexing in the batch (\"batch dimension\"):\n",
    "print(kernel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba2eae",
   "metadata": {
    "id": "9bba2eae"
   },
   "outputs": [],
   "source": [
    "# Converting the 'cat.jpg' image to torch.tensor:\n",
    "img = transforms.ToTensor()(image)\n",
    "img = img.unsqueeze(dim=0)  # add the \"batch dimension\"\n",
    "\n",
    "# Convolution:\n",
    "output = F.conv2d(img, kernel)\n",
    "fig = plt.figure()\n",
    "plt.imshow(output[0,0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e4b69",
   "metadata": {
    "id": "ce0e4b69"
   },
   "source": [
    "In practice, kernels of such large size are rarely used. For example, in a VGG, the kernels are of size 3*3. This is sufficient to extract interesting features, such as contours.\n",
    "\n",
    "**Exercise 1:** Apply a [Prewitt filter](https://en.wikipedia.org/wiki/Prewitt_operator) to the image using PyTorch's `conv2d` (complete the following code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc3a7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "deletable": false,
    "id": "b9cc3a7d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5bc0bf0a87eb5b4a5d64d1064a63eb",
     "grade": false,
     "grade_id": "exercise-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "8aa3b754-9ffb-4f22-fa0b-a7e2ca9f1b0b"
   },
   "outputs": [],
   "source": [
    "# kernel2 = ...\n",
    "# kernel2 = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#output1 = ..\n",
    "#output2 = ..\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "output = (output1**2 + output2**2).sqrt()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(output[0,0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8f81d",
   "metadata": {
    "id": "a5c8f81d"
   },
   "source": [
    "However, the Prewitt filter cannot be encoded in a standard neural network: it involves a square root. The nonlinearities implemented in these networks are simpler:\n",
    "\n",
    "- the ReLU function. It is simply the \"positive part\" function.\n",
    "- maxpooling. It is a form of downsampling. In its most common form, the image is divided into 2*2 pixel squares, and the maximum value in each square is returned. The spatial dimensions of the output tensor are thus divided by two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1094d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2e1094d",
    "outputId": "c3f9585e-fb76-4c27-f487-74ce5a6a1c33"
   },
   "outputs": [],
   "source": [
    "# Positive part: ReLU function\n",
    "x = torch.rand(1,1,4,4) - 0.5\n",
    "print(x)\n",
    "print(x.relu())\n",
    "\n",
    "# Maxpooling:\n",
    "x = F.max_pool2d(x, kernel_size=2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc793f3",
   "metadata": {
    "deletable": false,
    "id": "0dc793f3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79ff8e02ad8c85d9c3a110f3c13b902e",
     "grade": true,
     "grade_id": "exercise-2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**Exercise 2:** What is the size of the tensor at the output of layer 30 of vgg16, compared to that of the input image?\n",
    "YOUR ANSWER HERE\n",
    "**B.** Convolution Kernels After Training\n",
    "\n",
    "Let's now observe the convolutional layers after training on a very large set of annotated images (~1M) from the ImageNet database ([http://www.image-net.org/challenges/LSVRC/2010/index](https://)). First, let's see what happens to the kernels associated with the 64 neurons in the first convolutional layer of a pre-trained ResNet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef565b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1eef565b",
    "outputId": "3b32b7b7-694d-483d-8fd5-4dfa404947a2"
   },
   "outputs": [],
   "source": [
    "# Before training:\n",
    "model = models.resnet50(pretrained=False)\n",
    "first_layer = model.conv1.weight.data\n",
    "\n",
    "print(first_layer.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(first_layer.shape[0]):\n",
    "    plt.subplot(8, 8, i+1) #\n",
    "    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6943e5",
   "metadata": {
    "id": "5a6943e5"
   },
   "outputs": [],
   "source": [
    "# After training:\n",
    "model = models.resnet50(pretrained=True)\n",
    "first_layer = model.conv1.weight.data\n",
    "\n",
    "print(first_layer.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(first_layer.shape[0]):\n",
    "    plt.subplot(8, 8, i+1) #\n",
    "    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bddbb6",
   "metadata": {
    "id": "46bddbb6"
   },
   "source": [
    "Among these kernels, we find contour extractors similar to the Prewitt filter.  \n",
    "Signal processing enthusiasts will even recognize patterns very close to the [Morlet wavelets](https://www.google.com/search?q=wavelet+morlet+2d&rlz=1C1AVFC_enFR826FR857&hl=fr&sxsrf=ALeKk01sWHdzUO6bRogEv0KFx2gRgOWz_Q:1610077971021&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiIq5fst4vuAhXPxYUKHaskDqAQ_AUoAXoECAUQAw&biw=915&bih=591).  \n",
    "This is remarkable: through a simple gradient descent, effective filters for shape recognition have emerged.\n",
    "\n",
    "**Exercise 3:** How many \"neurons\" does the first convolutional layer of a ResNet50 have (one kernel per neuron)?  \n",
    "How many weights are there in a kernel?  \n",
    "How many weights does this layer contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96211075",
   "metadata": {
    "deletable": false,
    "id": "96211075",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aae270771ed643da5226e5e3ea5fbd83",
     "grade": true,
     "grade_id": "exercise-3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec98dfb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ec98dfb",
    "outputId": "48507ac6-8474-40ae-d2e8-2cab327e6bd0"
   },
   "outputs": [],
   "source": [
    "# This can be verified with the module.parameters() generator,\n",
    "# applicable to any instance of torch.nn\n",
    "nb_weights = 0\n",
    "for parameter in model.conv1.parameters():\n",
    "  # numel: counts the number of elements in a tensor\n",
    "  nb_weights += torch.numel(parameter)\n",
    "\n",
    "print(nb_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770555e",
   "metadata": {
    "id": "2770555e"
   },
   "source": [
    "**C.** Feature Maps\n",
    "\n",
    "Now let's see what happens to the image as it passes through the network. Let's take VGG16, pass it on our cat image, and see how the signal changes as it propagates through the network. First, let's see if the network recognizes the correct class among the thousand classes in the dataset. The list of classes is [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac79949",
   "metadata": {
    "id": "9ac79949"
   },
   "outputs": [],
   "source": [
    "image = Image.open(path).convert(\"RGB\")\n",
    "image = image.resize((256,256))\n",
    "\n",
    "img = transforms.ToTensor()(image)\n",
    "\n",
    "img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "img = img.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c7fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee2c7fdc",
    "outputId": "619729f8-29e6-409d-bd0c-b2fa275f045b"
   },
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c62e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "9c5c62e4",
    "outputId": "5681ee72-2913-4c32-ca7b-bd4f9ebf9864"
   },
   "outputs": [],
   "source": [
    "# Raw output\n",
    "output = model(img)\n",
    "\n",
    "# Softmax function\n",
    "output = output.softmax(dim=1).detach()\n",
    "\n",
    "# Prediction\n",
    "_, c  =torch.max(output, dim=1)\n",
    "print(c)\n",
    "\n",
    "# \"Probabilities\" associated with classes\n",
    "plt.plot(output.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a485a6",
   "metadata": {
    "id": "16a485a6"
   },
   "source": [
    "You may notice that by running the last cell several times, the output probabilities differ even though the input does not change. This is due to \"dropout\" (check `print(model)`), which deactivates a random subset of the classifier neurons (this operation helps combat **overfitting**).  \n",
    "\n",
    "To disable \"dropout\" and freeze the network, switch to \"eval\" mode with the command:  \n",
    "\n",
    "*model.eval()*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fd242",
   "metadata": {
    "id": "261fd242"
   },
   "source": [
    "Now, let's visualize the signal at the output of a convolutional layer. For this, we can use the [*hook*](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html) command. The channels of these intermediate signals are called **feature maps**. Let's see the feature maps associated with the first convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caff8f9",
   "metadata": {
    "id": "0caff8f9"
   },
   "outputs": [],
   "source": [
    "z = []\n",
    "# Function that stores in the list z the \"feature maps\"\n",
    "def store_layer_output(model, input, output):\n",
    "        z.append(output.detach())\n",
    "\n",
    "\n",
    "model.features[0].register_forward_hook(store_layer_output)\n",
    "model.features[10].register_forward_hook(store_layer_output)\n",
    "model.features[17].register_forward_hook(store_layer_output)\n",
    "model.features[28].register_forward_hook(store_layer_output)\n",
    "\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e531fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19e531fe",
    "outputId": "7b1b0ef8-7f6f-4525-cdf9-51eeb742e8b1"
   },
   "outputs": [],
   "source": [
    "for fm in z:\n",
    "  print(fm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ec5bb",
   "metadata": {
    "id": "8c3ec5bb"
   },
   "source": [
    "**Exercise 4:** Visualize the feature maps of layers 0, 10, 17, 28. You will notice that at level 10, the neuron's response is often specific to a particular trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125627f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "id": "e125627f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5675a175932ad90e49d4ba4f7c9b56fb",
     "grade": false,
     "grade_id": "exercise-4-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "1175019c-ef5a-4513-a35e-efc514f508d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_maps = z[1]\n",
    "\n",
    "print(feature_maps.shape)\n",
    "plt.figure(figsize=(20, 17))\n",
    "# for i in range(...):\n",
    "#    plt.subplot(...)\n",
    "#    plt.imshow(...)\n",
    "#    plt.axis('off') # don't show plot axis\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4086e2c",
   "metadata": {
    "id": "e4086e2c"
   },
   "source": [
    "Overlay on the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4919dd",
   "metadata": {
    "deletable": false,
    "id": "4c4919dd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97b755004638909c0d2544ef8f8ad43a",
     "grade": false,
     "grade_id": "exercise-4-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load image\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "f, axes = plt.subplots(8,8)\n",
    "for i in range(64):\n",
    "    ax = axes[i//8][i%8]\n",
    "    # fm1 = feature map\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    fm1 = cv2.resize(fm1, (image.size))\n",
    "    fm1 /= 0.5*np.max(fm1)\n",
    "    fm1 = np.uint8(fm1*255)\n",
    "    fm1 = cv2.applyColorMap(fm1,  cv2.COLORMAP_HOT)\n",
    "    fm1 = Image.fromarray(fm1)\n",
    "\n",
    "    superp = Image.blend(image, fm1, 0.5)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(superp)\n",
    "f.set_figheight(20)\n",
    "f.set_figwidth(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
